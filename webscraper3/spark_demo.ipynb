{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to start\n",
    "First we have to create a spark sessionand we have to give it an appname and a master\n",
    "the master will tell spark where the spark job needs to run, here it will be local because we are running it on our local machine.\n",
    "Typically you would put an url for a spark cluster/mesos cluster or YARN if you were using that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"demo application\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a dataframe to interact with (structured data in our case)\n",
    "we read it from a csv but if it is an array you can use the parelilize method to create a dataframe from an array.\n",
    "we need to also pass a schema when creating a dataframe (column names) our csv already has the headers so it will do it automatically also datatypes will be asssigned automatically because we give the option inferschema\n",
    "\n",
    "the \\ are used so that cwe can make the code more readable since laot of .methods are being chained behind eachother.\n",
    "\n",
    "we have to specify the path and then load the data into a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.format('csv').option('inferSchema', 'True').\\\n",
    "    option('header', 'True').\\\n",
    "        option('path', './data/operations_management.csv').\\\n",
    "            load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- description: string (nullable = true)\n",
      " |-- industry: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- line_code: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see it has created the dataframe with the correct datatypes for every column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMING DATA\n",
    "like in pandas, you do not change the original data in the dataframe.\n",
    "\n",
    "here we will select two columns, industry and value because in this example we are interested in that.\n",
    "\n",
    "we filter so that we only have the industries that are worth over 1000 and then we sort them in descending order to get the most valuable ones first in this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ac opy and apply transformation logic  to the dataframe\n",
    "data2 = data.select(\"industry\", \"value\").\\\n",
    "    filter(data[\"value\"] > 10000).\\\n",
    "    orderBy(data[\"value\"].desc())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|industry|value|\n",
      "+--------+-----+\n",
      "|   total|41091|\n",
      "|   total|40431|\n",
      "|   total|33984|\n",
      "|   total|33750|\n",
      "|   total|32652|\n",
      "|   total|31134|\n",
      "|   total|30624|\n",
      "|   total|30543|\n",
      "|   total|30396|\n",
      "|   total|30099|\n",
      "|   total|29994|\n",
      "|   total|29775|\n",
      "|   total|28473|\n",
      "|   total|27846|\n",
      "|   total|26010|\n",
      "|   total|25977|\n",
      "|   total|25887|\n",
      "|   total|25500|\n",
      "|   total|25434|\n",
      "|   total|25221|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the industry column is showing \"total\" for alot of rows which means the dataset had columns mixed with total values of certain industries mixed with the actual industry names, we need to filter this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.select(\"industry\", \"value\").\\\n",
    "    filter((data[\"value\"] > 2000) & (data[\"industry\"] != \"total\")).\\\n",
    "    filter((data[\"value\"] > 2000) & (data[\"industry\"] != \"total\")).\\\n",
    "    orderBy(data[\"value\"].desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            industry|value|\n",
      "+--------------------+-----+\n",
      "|        Construction| 6030|\n",
      "|        Construction| 5904|\n",
      "|        Construction| 5229|\n",
      "|Accommodation & f...| 5058|\n",
      "|        Construction| 4965|\n",
      "|        Construction| 4959|\n",
      "|Accommodation & f...| 4950|\n",
      "|        Construction| 4686|\n",
      "|        Construction| 4668|\n",
      "|        Construction| 4665|\n",
      "|       Manufacturing| 4662|\n",
      "|       Manufacturing| 4632|\n",
      "|        Construction| 4575|\n",
      "|        Construction| 4566|\n",
      "|Professional, sci...| 4476|\n",
      "|Professional, sci...| 4470|\n",
      "|        Retail trade| 4434|\n",
      "|        Retail trade| 4434|\n",
      "|Accommodation & f...| 4251|\n",
      "|Accommodation & f...| 4176|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is alot of duplicate industries, these are probably tied to different company names but we want the value for the industries themselves, so we need to group by the industry name.\n",
    "\n",
    "to do that we will need to import aggregation functions from the pyspark library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in SQL you would put the aggregation function in the select statement however in pyspark you have a seperate clause for it after the groupby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.select(\"industry\", \"value\").\\\n",
    "    filter((data[\"value\"] > 2000) & (data[\"industry\"] != \"total\")).\\\n",
    "    groupBy(\"industry\").\\\n",
    "    agg(_sum(\"value\").alias(\"total_industry_value\")).\\\n",
    "    orderBy(\"total_industry_value\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+--------------------+\n",
      "|industry                                      |total_industry_value|\n",
      "+----------------------------------------------+--------------------+\n",
      "|Construction                                  |177639              |\n",
      "|Accommodation & food services                 |150768              |\n",
      "|Professional, scientific, & technical services|114789              |\n",
      "|Manufacturing                                 |100275              |\n",
      "|Retail trade                                  |76461               |\n",
      "|Other professional scientific                 |58602               |\n",
      "|Agriculture, forestry, & fishing              |38547               |\n",
      "|Health care & social assistance               |17601               |\n",
      "|Wholesale trade                               |16431               |\n",
      "|Agriculture                                   |4149                |\n",
      "+----------------------------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL in apache spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL queries will not run directly on apache spark dataframes themselves, you will need to create a view or temporary view from the dataframe in order to do SQL on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView(\"data2_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when creating a temporary view, it will ge tregistered in the spark SQL catalog and any SQL queries using the spark.sql() method will reference this view you created by the name you gave that view and will show up as a temporary table in the spark catalog if you call listtables()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='data', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True), Table(name='data2_view', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True), Table(name='data3', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True), Table(name='data4', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "print(spark.catalog.listTables())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+-----+\n",
      "|industry                                      |value|\n",
      "+----------------------------------------------+-----+\n",
      "|Construction                                  |6030 |\n",
      "|Construction                                  |5904 |\n",
      "|Construction                                  |5229 |\n",
      "|Accommodation & food services                 |5058 |\n",
      "|Construction                                  |4965 |\n",
      "|Construction                                  |4959 |\n",
      "|Accommodation & food services                 |4950 |\n",
      "|Construction                                  |4686 |\n",
      "|Construction                                  |4668 |\n",
      "|Construction                                  |4665 |\n",
      "|Manufacturing                                 |4662 |\n",
      "|Manufacturing                                 |4632 |\n",
      "|Construction                                  |4575 |\n",
      "|Construction                                  |4566 |\n",
      "|Professional, scientific, & technical services|4476 |\n",
      "|Professional, scientific, & technical services|4470 |\n",
      "|Retail trade                                  |4434 |\n",
      "|Retail trade                                  |4434 |\n",
      "|Accommodation & food services                 |4251 |\n",
      "|Accommodation & food services                 |4176 |\n",
      "+----------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT industry, value \n",
    "          FROM data2_view \n",
    "          WHERE industry != 'total' AND value > 2000\n",
    "          ORDER BY value DESC\n",
    "          \"\"\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView(\"data2_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+--------------------+\n",
      "|industry                                      |total_industry_value|\n",
      "+----------------------------------------------+--------------------+\n",
      "|Construction                                  |177639              |\n",
      "|Accommodation & food services                 |150768              |\n",
      "|Professional, scientific, & technical services|114789              |\n",
      "|Manufacturing                                 |100275              |\n",
      "|Retail trade                                  |76461               |\n",
      "|Other professional scientific                 |58602               |\n",
      "|Agriculture, forestry, & fishing              |38547               |\n",
      "|Health care & social assistance               |17601               |\n",
      "|Wholesale trade                               |16431               |\n",
      "|Agriculture                                   |4149                |\n",
      "+----------------------------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT industry, SUM(value) AS total_industry_value \n",
    "          FROM data2_view \n",
    "          WHERE industry != 'total' AND value > 2000\n",
    "          GROUP BY industry\n",
    "          ORDER BY total_industry_value DESC\n",
    "          \"\"\").show(truncate=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that is the basics of what you can do with dataprocessing using apache spark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
